{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Architecture\n",
    "# https://www.tensorflow.org/extend/architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hands on useful core OPs of Tensorflow\n",
    "# tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (1) Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sess = tf.Session() - returns graph_pb2.GraphDef contatining nodes definition\n",
    "# sess.run()\n",
    "# sess.close()\n",
    "\n",
    "# Using the context manager which modifies default session to the lifetime of context\n",
    "# with tf.Session() as sess:\n",
    "#   sess.run()\n",
    "#   variable.eval() - shows that the operation is evaluating in this session\n",
    "\n",
    "# sess.as_default() - Returns a context manager that makes this object the default session\n",
    "# with sess.as_default():\n",
    "#   assert tf.get_default_session() is sess\n",
    "# tf.get_default_session - Gets current default session\n",
    "\n",
    "# sess.run(fetches, feed_dict={}) exexutes a graph registered with this session\n",
    "# fetches - can be tensor, string, operation\n",
    "# returns list of fetches values\n",
    "\n",
    "# sess = tf.InteractiveSession() - installs itself as the default session. We don't have to define it again and again \n",
    "# The methods tf.Tensor.eval and tf.Operation.run will use that session to run ops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (2) Graphs (quick overview)\n",
    "# A Graph contains a set of tf.Operation objects, which represent units of computation and \n",
    "# tf.Tensor objects, which represent the units of data that flow between operations.\n",
    "\n",
    "# A default Graph is always registered, and accessible by calling tf.get_default_graph\n",
    "# g.default_graph() - default graph can be overridden by this context manager which \n",
    "# overrides the current default graph for the lifetime of the context\n",
    "# Use this method with the with keyword to specify that ops \n",
    "# created within the scope of a block should be added to this graph only and no other graph\n",
    "# this allows us to exectute more than one graph simultaneously\n",
    "\n",
    "# g = tf.Graph()\n",
    "# with g.as_default():\n",
    "#   # Define operations and tensors in `g`.\n",
    "#   c = tf.constant(30.0)\n",
    "#   assert c.graph is g\n",
    "\n",
    "# graph supports collections - collections can store groups of related objects like losses, variables etc\n",
    "# GraphKeys - https://www.tensorflow.org/api_docs/python/tf/GraphKeys\n",
    "# add_to_collection(name, value) - adds new data item to its list of available collections\n",
    "\n",
    "# .as_graph_def() - Returns a serialized GraphDef representation of this graph.\n",
    "\n",
    "# .as_graph_element(obj) - Returns the object referred to by obj, as an Operation or Tensor.\n",
    "# This function validates that obj represents an element of this graph, and gives error if not\n",
    "\n",
    "# clear_collection(name) - clears the named value\n",
    "# get_all_collection_keys() - gets the list of collections\n",
    "# get_collection(name, scope=None) - gets the value for named key, always creates a copy of named key and returns\n",
    "# get_collection_ref() - always returns original list\n",
    "\n",
    "# colocate_with() - Returns a context manager that specifies an op to colocate with.\n",
    "# a = tf.Variable([1.0])\n",
    "# with g.colocate_with(a):\n",
    "#   b = tf.constant(1.0)\n",
    "#   c = tf.add(a, b)\n",
    "# here whereever a is present it will always be associated with b and c\n",
    "\n",
    "# container() - stateful operations like queues and variables can maintain there states and can be used across\n",
    "# devices with same states. so we create container for similar types and track there states with them\n",
    "# with g.container('variable_add'):\n",
    "#   v1 = tf.Variable([1.0])\n",
    "#   v2 = tf.Variable([2.0])\n",
    "#   v3 = v1 + v2\n",
    "#   v4 = v1 + 10\n",
    "# with g.container('variable_mul'):\n",
    "#   v5 = tf.Variable([11.0])\n",
    "#   v6 = tf.Variable([32.0])\n",
    "#   v7 = v5 * v6\n",
    "#   v8 = v1 * 8\n",
    "\n",
    "# control_dependencies(control_inputs) - all operations constructed within this context should run only after\n",
    "# control_inputs has been exececuted first\n",
    "# with g.control_dependencies([a, b, c]):\n",
    "#   d = d + 10\n",
    "#   e = e * d\n",
    "# pass None to clear control dependency in case of nested dependencies.\n",
    "\n",
    "# .Device() - default device to use\n",
    "#. get_operation_by_name(name) - Returns the Operation with the given name.\n",
    "# .get_tensor_by_name(name) - Returns the Tensor with the given name.\n",
    "\n",
    "# GraphKeys - https://www.tensorflow.org/api_docs/python/tf/GraphKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (3) Sharing Variables\n",
    "# .name_scope(*args, **kwds) - creates a stack of hierarchical names of operations\n",
    "# .variable_scope() - creates a stack of hierarchical names of variables\n",
    "# http://stackoverflow.com/questions/35919020/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow\n",
    "# this basically ensures that rather than variables are created everytime, old ones are used as it is to ensure proper \n",
    "# weight sharing between operations/graphs \n",
    "# http://stackoverflow.com/questions/34592172/about-names-of-variable-scope-in-tensorflow\n",
    "# http://stackoverflow.com/questions/33725159/understanding-variable-scope-example-in-tensorflow\n",
    "# retrieving a variable using its scope\n",
    "# with tf.variable_scope(\"param\"):\n",
    "#    w = tf.get_variable(\"weights\", [1])\n",
    "#    print(\"w:\", w.name)\n",
    "# name_scope() - on tf.Variable only\n",
    "# variable_scope() - on tf.get_variable also along with tf.Variable\n",
    "# https://www.tensorflow.org/programmers_guide/variable_scope\n",
    "# tf.get_variable_scope().reuse_variables() - for reusing same variable instead of re-initalising it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (4) Variables\n",
    "# https://www.tensorflow.org/api_guides/python/state_ops#Exporting_and_Importing_Meta_Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (5) Some useful operations\n",
    "# tf.argmax() - returns max along axis - used in predictions\n",
    "# tf.one_hot() - encodes labels to 1 rest to 0 by taking index and depth of tensor\n",
    "# tf.train.write_graph(sess.graph, '/tmp/my-model', 'train.pbtxt') -  writes the graph's pbtxt file\n",
    "# tf.train.global_step()\n",
    "# tf.train.get_global_step()\n",
    "# tf.losses.softmax_cross_entropy(onehot_labels, logits, loss_collection=tf.GraphKeys.LOSSES)\n",
    "# tf.losses.get_losses() - Gets the list of losses from the loss_collection.\n",
    "# tf.losses.get_total_loss(add_regularization_losses=True, name='total_loss') - Returns a tensor whose value represents the total loss.\n",
    "# tf.losses.sigmoid_cross_entropy(multi_class_labels, logits, weights=1.0) - same as loss but calculates sigmoid loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (6) Saver\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/Saver\n",
    "# stores and retrieves lastes checkpoint\n",
    "# saver = tf.Saver()\n",
    "# saver.save(sess, 'my-model', global_step=0) # global step is optional # returns path that can be passed to restore\n",
    "# saver.restore() # restores the last checkpoint \n",
    "# tf.train.latest_checkpoint(checkpoint_dir, latest_filename=None) - grab latest checkpoint\n",
    "# tf.train.get_checkpoint_state(checkpoint_dir, latest_filename=None) - gets latest state from checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (7) Summary\n",
    "#  tf.summary.FileWriter - writes a summary to event file. Updates asynchronously.\n",
    "# This event file will contain Event protocol buffers constructed when you call one of the following functions: \n",
    "# add_summary(), add_session_log(), add_event(), or add_graph().\n",
    "# writer = tf.summary.FileWriter(<some-directory>, sess.graph)\n",
    "# writer.add_session_log(session_log, global_step=None) - adds session log\n",
    "# tf.summary.tensor_summary - adds tensor summary as serialized proto\n",
    "# tf.summary.scalar() - adds single scalar unit  like loss, accuracy etc.\n",
    "# tf.summary.histogram() - when we want to add list of data to see it overtime like activation fuctions etc\n",
    "# similarly image, audio\n",
    "# all summaries are added to GraphKeys.SUMMARIES\n",
    "# tf.summary.merge - merges all the summary op into one\n",
    "# tf.summary.merge_all - Merges all summaries collected in the default graph in GraphKeys.SUMMARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (8) Most used operations - \n",
    "# https://www.tensorflow.org/api_guides/python/train - gradient optimizers\n",
    "# https://www.tensorflow.org/api_guides/python/nn - layers used func such as convd, relu etc\n",
    "# https://www.tensorflow.org/api_docs/python/tf/losses - losses to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (9) optimizers\n",
    "# operations that are responsible for learning\n",
    "# The Optimizer base class provides methods to compute gradients for a loss and apply gradients to trainable variables\n",
    "# every obtimizer is associated with minimize() property that is responsible fo calculating and updating gradients of \n",
    "# variable. gate_gradients (optional) - IT has GATE_NONE, GATE_OP, GATE_GRAPH for parallelism. Default is GraphKey.TRAINABLE_VARIABLES\n",
    "\n",
    "# loss = tf.reduce_mean(tf.square(layer2 - marks)) or tf.losses.mean_squared_error(labels=layer2, predictions=marks)\n",
    "# optimizer = tf.train.AdamOptimizer(0.5)\n",
    "# train = optimizer.minimize(loss)\n",
    "\n",
    "# If you want custom then do this - \n",
    "# 1) create optimizer\n",
    "# 2) pass the list to for optimizing, default is list of all trainable variables\n",
    "# 3) compute gradients - if you want you own computation else leave it on algo\n",
    "# 4) apply gradients - to apply the gradients across variables\n",
    "\n",
    "# opt = GradientDescentOptimizer(learning_rate=0.1)\n",
    "# grads_and_vars = opt.compute_gradients(loss, <list of variables>)\n",
    "# capped_grads_and_vars = [(MyCapper(gv[0]), gv[1]) for gv in grads_and_vars]\n",
    "# opt.apply_gradients(capped_grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (10) weight decay-\n",
    "# decays the learning rate  while training - \n",
    "# tf.train.exponential_decay\n",
    "# tf.train.inverse_time_decay\n",
    "# tf.train.natural_exp_decay\n",
    "# tf.train.piecewise_constant\n",
    "# tf.train.polynomial_decay\n",
    "\n",
    "# learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 100000, 0.96, staircase=True)\n",
    "# tf.train.GradientDescentOptimizer(learning_rate).minimize(...my loss..., global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
