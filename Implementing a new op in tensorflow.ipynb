{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a new op in tensorflow          \n",
    "#####      [Hands on time - 40min]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why its needed?\n",
    "1) When you cannot express your operation as combination of existing operations\n",
    "<br>2) When the operation is not available. Example - Sparse Convolution Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1) Register the new op in c++ file. Basically write an interface. <br>\n",
    "2) Write the operation's logic.<br>\n",
    "3) Build the operation to generate its .so file.<br>\n",
    "4) Set .so path's in LD_LIBRARY_PATH.<br>\n",
    "5) Test if it works in python.<br>\n",
    "6) You can write a python wrapper on top of it if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Register the new op in c++ file. Basically write an interface. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#include \"tensorflow/core/framework/op.h\"\n",
    "#include \"tensorflow/core/framework/shape_inference.h\"\n",
    "#include \"tensorflow/core/framework/op_kernel.h\"\n",
    "\n",
    "using namespace tensorflow;\n",
    "\n",
    "REGISTER_OP(\"ZeroOut\")\n",
    "    .Input(\"to_zero: int32\")\n",
    "    .Output(\"zeroed: int32\")\n",
    "    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n",
    "      c->set_output(0, c->input(0));\n",
    "      return Status::OK();\n",
    "    });"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### op.h \n",
    "OpRegistryInterface which is responsible for registering the operation.<br>\n",
    "Before that there is watcher which watches a particular operation is already registered<br>\n",
    "if yes, then returns non-ok status else registers its returning pointer which is then used further.<br>\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shape_inference.h\n",
    "It uses InferenceContext by shape inference function which is used for inferencing/setting the shape of output tensors.\n",
    "It uses set_output() fucntion which specifies the shape of our output function. In above example shape of input and outputs are same.\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/shape_inference.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operation name must be in camel case. Why? Later on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Write the operation's logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opKernal.h\n",
    "opKernal provides base function compute() which needs to be overridden with your implementation. It can be both synchronous and Asynchronous. Its implemented in thread-safe way so that many process can use it concurrently.It takes a input tensor which can be accessed using OpKernelContext*\n",
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ZeroOutOp : public OpKernel {\n",
    " public:\n",
    "  explicit ZeroOutOp(OpKernelConstruction* context) : OpKernel(context) {}\n",
    "\n",
    "  void Compute(OpKernelContext* context) override {\n",
    "    // Grab the input tensor\n",
    "    const Tensor& input_tensor = context->input(0);\n",
    "    auto input = input_tensor.flat<int32>();\n",
    "\n",
    "    // Create an output tensor\n",
    "    Tensor* output_tensor = NULL;\n",
    "    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(), &output_tensor)); // to allocate a new output buffer\n",
    "    auto output = output_tensor->flat<int32>();\n",
    "\n",
    "    // Set all but the first element of the output tensor to 0.\n",
    "    const int N = input.size();\n",
    "    for (int i = 1; i < N; i++) {\n",
    "      output(i) = 0;\n",
    "    }\n",
    "\n",
    "    // Preserve the first input value if possible.\n",
    "    if (N > 0) output(0) = input(0);\n",
    "  }\n",
    "};\n",
    "\n",
    "REGISTER_KERNEL_BUILDER(Name(\"ZeroOut\").Device(DEVICE_CPU), ZeroOutOp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGISTER_KERNEL_BUILDER\n",
    "Final stage of registration process. This is used for setting the execute mode of the operation. Ex CPU or GPU.<br>\n",
    "This is used when you have same kernal but one for CPU and one for GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Build the operation to generate its .so file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Using gcc\n",
    "\n",
    "Find the path of your tensorflow include directory using:<br>\n",
    "<i>import tensorflow as tf; print(tf.sysconfig.get_include())</i><br>\n",
    "TF_INC= \"/usr/local/lib/python2.7/dist-packages/tensorflow/include\"<br>\n",
    "g++ -std=c++11 -shared zero_out.cc -o zero_out.so -fPIC -I $TF_INC -O2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Using Bazel\n",
    "\n",
    "save BUILD file with following configuration :<br>\n",
    "load(\"//tensorflow:tensorflow.bzl\", \"tf_custom_op_library\")<br>\n",
    "tf_custom_op_library(\n",
    "    name = \"zero_out.so\",\n",
    "    srcs = [\"zero_out.cc\"],\n",
    ")\n",
    "\n",
    "bazel build --config opt //tensorflow/core/user_ops:zero_out.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Set .so path's in LD_LIBRARY_PATH.\n",
    "\n",
    "This is very important or else tensorflow won't be able to load this .so file. <br>\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib/python2.7/dist-packages/tensorflow/include/zero_out.so"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Test if it works in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "zero_out_module = tf.load_op_library('zero_out.so')\n",
    "with tf.Session(''):\n",
    "  zero_out_module.zero_out([[1, 2], [3, 4]]).eval()\n",
    "\n",
    "# Prints\n",
    "array([[1, 0], [0, 0]], dtype=int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.load_op_library\n",
    "\n",
    "Function to load the dynamic library and register the op with the TensorFlow framework. load_op_library returns a Python module that contains the Python wrappers for the op and the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this #Operation-name-must-be-in-camel-case.-Why?-Later-on!<br>\n",
    "Wrapper automatically converts the Title case to snake case complying with PEP8. No additional effort to write a wrapper for.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
